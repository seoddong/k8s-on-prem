{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5dbf932-1a90-4691-ac48-aeb4f7ce8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b236f3bc-6cfa-43ee-8fcd-92fb3727eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "SparkConf()\n",
    "    .setAppName(\"MY_kafka\") # replace with your desired name\n",
    "    .set(\"spark.jars.packages\", \\\n",
    "         \"org.apache.commons:commons-pool2:2.11.1\"\\\n",
    "         \",org.apache.kafka:kafka-clients:3.4.0\"\\\n",
    "         \",org.apache.spark:spark-protobuf_2.12:3.4.1\"\\\n",
    "         \",org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\"\\\n",
    "         \",org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.4.1\"\\\n",
    "        )\n",
    "    # .set(\"spark.sql.shuffle.partitions\", \"4\") # default is 200 partitions which is too many for local\n",
    "    # .setMaster(\"local[*]\") # replace the * with your desired number of cores. * for use all.\n",
    "    .setMaster(\"spark://34.125.136.103:30077\")\n",
    "    # .set(\"spark.driver.host\", \"34.125.136.103\") # 원격 실행 시 exited with code 1 exitStatus 1(무한재시작) 현상 방지용\n",
    "    # .set(\"spark.driver.port\", \"18080\") # 원격 실행 시 exited with code 1 exitStatus 1(무한재시작) 현상 방지용\n",
    "    # .set(\"spark.driver.bindAddress\", \"0.0.0.0\") # 원격 실행 시 exited with code 1 exitStatus 1(무한재시작) 현상 방지용\n",
    "    # .set(\"spark.shuffle.service.enabled\", \"false\") # 원격 실행 시 Initial job has not accepted any resources 현상 방지용\n",
    "    # .set(\"spark.dynamicAllocation.enabled\", \"false\") # 원격 실행 시 Initial job has not accepted any resources 현상 방지용\n",
    "    # .set(\"spark.dynamicAllocation.enabled\", \"false\") # 원격 실행 시 Initial job has not accepted any resources 현상 방지용\n",
    "    # .set(\"spark.driver.memory\", \"2g\")  # driver 메모리 설정\n",
    "    # .set(\"spark.executor.memory\", \"2g\")  # executor 메모리 설정\n",
    "    # .set(\"spark.executor.instances\", \"2\")  # executor 인스턴스 수 설정\n",
    "    # .set(\"spark.executor.cores\", \"2\")  # executor 코어 수 설정\n",
    "\n",
    "    # 아래는 클러스터 모드로 작동하는 법이라고 하는데 여기선 사용 불가함\n",
    "    # .setMaster(\"k8s://https://34.125.136.103:6443\")\n",
    "    # .set(\"spark.kubernetes.container.image\", \"bitnami/spark:3\")\n",
    "    # .set(\"spark.kubernetes.driverEnv.SPARK_MASTER_URL\", \"spark://34.125.136.103:30077\")\n",
    "    # .set(\"spark.submit.deployMode\", \"cluster\")  # 배포 모드 설정 (클라이언트에서 실행 시 cluster모드 선택 불가)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7c7beb-dec8-4582-9843-7991c6587128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/root/miniconda3/envs/spark341/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.commons#commons-pool2 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      "org.apache.spark#spark-protobuf_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-token-provider-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bb8761e0-c432-4b4f-9442-4b50f1ff33bc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.2-1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound org.apache.spark#spark-protobuf_2.12;3.4.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      ":: resolution report :: resolve 1406ms :: artifacts dl 30ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.5.2-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-protobuf_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 by [org.xerial.snappy#snappy-java;1.1.10.1] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.6] in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 by [org.apache.kafka#kafka-clients;3.4.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   3   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bb8761e0-c432-4b4f-9442-4b50f1ff33bc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/16ms)\n",
      "24/04/25 17:58:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf)\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a830953a-0cb1-4bfd-8324-5f0ed363145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_servers = 'peter-kafka01.foo.bar:9092,peter-kafka02.foo.bar:9092,peter-kafka03.foo.bar:9092'\n",
    "topic = 'airportcodes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f11a38-4efb-4e48-95e9-b527e128bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a79bf3ad-4e7a-4d5a-9df1-f945779c66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"ident\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"elevation_ft\", StringType(), True),\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"iso_country\", StringType(), True),\n",
    "    StructField(\"iso_region\", StringType(), True),\n",
    "    StructField(\"municipality\", StringType(), True),\n",
    "    StructField(\"gps_code\", StringType(), True),\n",
    "    StructField(\"iata_code\", StringType(), True),\n",
    "    StructField(\"local_code\", StringType(), True),\n",
    "    StructField(\"coordinates\", StringType(), True),\n",
    "])\n",
    "\n",
    "json_df = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd591ad9-2f09-41be-bc2e-fc13a28a4c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 17:58:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-51b02263-7b60-4089-93a3-a50288359318. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/25 17:58:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query = json_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44d96c55-a3b7-4d9a-921c-799b67ec0b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query.awaitTermination()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
