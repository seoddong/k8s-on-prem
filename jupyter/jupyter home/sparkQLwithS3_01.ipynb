{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e00725-f9a1-4a2c-93f5-5ff4e68ae915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04573b4a-5ea7-4c80-a4cf-0c80b7ed8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# %pip install python-dotenv\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53eeb4a2-6a7a-4732-9551-2ccbb517d747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('./env/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "616c7c8b-1d2b-4d27-a24c-6f1926645ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session with your AWS Credentials\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"sparkQLwithS3\") # replace with your desired name\n",
    "    .set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0,org.apache.hadoop:hadoop-aws:3.3.2\")\n",
    "    .set(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    # .set(\"spark.sql.shuffle.partitions\", \"4\") # default is 200 partitions which is too many for local\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "    # .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.profile.ProfileCredentialsProvider\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", os.environ['aws_access_key_id'])\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", os.environ['aws_secret_access_key'])\n",
    "    .setMaster(\"spark://34.125.136.103:30077\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6249e177-bd65-4509-a24e-df9a8deb3470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: com.amazonaws.services.s3.enableV4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/root/miniconda3/envs/spark341/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cb60f1bc-3d9b-406f-8891-da4651d097b7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 676ms :: artifacts dl 62ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cb60f1bc-3d9b-406f-8891-da4651d097b7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/13ms)\n",
      "24/04/25 17:12:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/25 17:12:58 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "df = spark.read.format('json').load('s3a://jolajoayo-spark-0001/spark2-sql/airports/airport-codes.csv.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8bba0d4-c30c-4dd4-a0a1-5fd2eb9fdb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------+---------+-----+-----------+----------+------------+----------+-------------+------------+----------------------------------+-------------+\n",
      "|continent|elevation_ft|gps_code|iata_code|ident|iso_country|iso_region|latitude_deg|local_code|longitude_deg|municipality|name                              |type         |\n",
      "+---------+------------+--------+---------+-----+-----------+----------+------------+----------+-------------+------------+----------------------------------+-------------+\n",
      "|NA       |11          |00A     |         |00A  |US         |US-PA     |40.07080078 |00A       |-74.93360138 |Bensalem    |Total Rf Heliport                 |heliport     |\n",
      "|NA       |450         |00AK    |         |00AK |US         |US-AK     |59.94919968 |00AK      |-151.6959991 |Anchor Point|Lowell Field                      |small_airport|\n",
      "|NA       |820         |00AL    |         |00AL |US         |US-AL     |34.8647995  |00AL      |-86.77030182 |Harvest     |Epps Airpark                      |small_airport|\n",
      "|NA       |237         |00AR    |         |00AR |US         |US-AR     |35.6086998  |00AR      |-91.25489807 |Newport     |Newport Hospital & Clinic Heliport|heliport     |\n",
      "|NA       |3810        |00AZ    |         |00AZ |US         |US-AZ     |34.30559921 |00AZ      |-112.1650009 |Cordes      |Cordes Airport                    |small_airport|\n",
      "+---------+------------+--------+---------+-----+-----------+----------+------------+----------+-------------+------------+----------------------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f5a21f-eac1-4e2d-8f32-7779ec21f761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- latitude_deg: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- longitude_deg: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c633354a-67f3-44f9-b78f-113165a5ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\n",
      "ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"airportcodes\")\n",
    "viewdf = spark.sql(\\\n",
    "                   \"SELECT iso_region, name, type \"\\\n",
    "                   \"FROM airportcodes \"\\\n",
    "                   \"WHERE iso_country = 'US' \"\\\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c95691-9939-41a2-919f-9173fd7a3a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+\n",
      "|iso_region|                name|         type|\n",
      "+----------+--------------------+-------------+\n",
      "|     US-PA|   Total Rf Heliport|     heliport|\n",
      "|     US-AK|        Lowell Field|small_airport|\n",
      "|     US-AL|        Epps Airpark|small_airport|\n",
      "|     US-AR|Newport Hospital ...|     heliport|\n",
      "|     US-AZ|      Cordes Airport|small_airport|\n",
      "|     US-CA|Goldstone /Gts/ A...|small_airport|\n",
      "|     US-CO|          Cass Field|small_airport|\n",
      "|     US-FL| Grass Patch Airport|small_airport|\n",
      "|     US-FL|  Ringhaver Heliport|     heliport|\n",
      "|     US-FL|   River Oak Airport|small_airport|\n",
      "|     US-GA|    Lt World Airport|small_airport|\n",
      "|     US-GA|    Caffrey Heliport|     heliport|\n",
      "|     US-HI|  Kaupulehu Heliport|     heliport|\n",
      "|     US-ID|Delta Shores Airport|small_airport|\n",
      "|     US-IN|Bailey Generation...|     heliport|\n",
      "|     US-IL|      Hammer Airport|small_airport|\n",
      "|     US-IN|St Mary Medical C...|     heliport|\n",
      "|     US-IL|Hayenga's Cant Fi...|small_airport|\n",
      "|     US-KS| Hayden Farm Airport|small_airport|\n",
      "|     US-KY|Robbins Roost Air...|small_airport|\n",
      "+----------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "viewdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434b4f04-bb3c-4642-b611-23a41c428ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 17:13:28 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "viewdf.write.format('csv').option('header','true').\\\n",
    "save('s3a://jolajoayo-spark-0001/spark2-sql/airports/airportcodes.csv',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf0292d-56e4-429a-a4a3-1d475ccfb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
