{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274adaf1-db1a-4db4-bb92-d1518a699544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# boto3를 통해 자동으로 자격증명 가져오기\n",
    "# ~/.aws/credentials를 미리 만들어놔야 함\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "aws_access_key_id = credentials.access_key\n",
    "aws_secret_access_key = credentials.secret_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa24eada-5ae2-48a4-ac3b-090cd21480a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: com.amazonaws.services.s3.enableV4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/root/miniconda3/envs/spark341/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.mariadb.jdbc#mariadb-java-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5ec788b3-6315-48b7-bdb3-4c3ada0e091a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.mariadb.jdbc#mariadb-java-client;3.3.3 in central\n",
      "\tfound com.github.waffle#waffle-jna;3.3.0 in central\n",
      "\tfound net.java.dev.jna#jna;5.13.0 in central\n",
      "\tfound net.java.dev.jna#jna-platform;5.13.0 in central\n",
      "\tfound org.slf4j#jcl-over-slf4j;2.0.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.3 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.10.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.32.0 in central\n",
      ":: resolution report :: resolve 1035ms :: artifacts dl 52ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.3 from central in [default]\n",
      "\tcom.github.waffle#waffle-jna;3.3.0 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.10.0 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\tnet.java.dev.jna#jna;5.13.0 from central in [default]\n",
      "\tnet.java.dev.jna#jna-platform;5.13.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.32.0 from central in [default]\n",
      "\torg.mariadb.jdbc#mariadb-java-client;3.3.3 from central in [default]\n",
      "\torg.slf4j#jcl-over-slf4j;2.0.7 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.checkerframework#checker-qual;3.19.0 by [org.checkerframework#checker-qual;3.32.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   1   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5ec788b3-6315-48b7-bdb3-4c3ada0e091a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/18ms)\n",
      "24/04/29 21:39:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session with your AWS Credentials\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"sparkQLwithS3\") # replace with your desired name\n",
    "    .set(\"spark.jars.packages\", \\\n",
    "         \"io.delta:delta-core_2.12:2.3.0\"\\\n",
    "         \",org.apache.hadoop:hadoop-aws:3.3.2\"\\\n",
    "         \",org.mariadb.jdbc:mariadb-java-client:3.3.3\"\\\n",
    "        )\n",
    "    .set(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"4\") # default is 200 partitions which is too many for local\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "    # .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.profile.ProfileCredentialsProvider\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", aws_access_key_id)\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_access_key)\n",
    "    .setMaster(\"spark://34.125.136.103:30077\")\n",
    ")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a22a54-3ac8-4e56-9750-ad1a8873ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/29 21:39:40 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('json').load('s3a://jolajoayo-spark-0001/spark2-sql/airports/airport-codes.csv.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afec81ce-99e8-4ac3-8d7b-eaba5a707b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\n",
      "ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+\n",
      "|iso_region|                name|         type|\n",
      "+----------+--------------------+-------------+\n",
      "|     US-PA|   Total Rf Heliport|     heliport|\n",
      "|     US-AK|        Lowell Field|small_airport|\n",
      "|     US-AL|        Epps Airpark|small_airport|\n",
      "|     US-AR|Newport Hospital ...|     heliport|\n",
      "|     US-AZ|      Cordes Airport|small_airport|\n",
      "|     US-CA|Goldstone /Gts/ A...|small_airport|\n",
      "|     US-CO|          Cass Field|small_airport|\n",
      "|     US-FL| Grass Patch Airport|small_airport|\n",
      "|     US-FL|  Ringhaver Heliport|     heliport|\n",
      "|     US-FL|   River Oak Airport|small_airport|\n",
      "|     US-GA|    Lt World Airport|small_airport|\n",
      "|     US-GA|    Caffrey Heliport|     heliport|\n",
      "|     US-HI|  Kaupulehu Heliport|     heliport|\n",
      "|     US-ID|Delta Shores Airport|small_airport|\n",
      "|     US-IN|Bailey Generation...|     heliport|\n",
      "|     US-IL|      Hammer Airport|small_airport|\n",
      "|     US-IN|St Mary Medical C...|     heliport|\n",
      "|     US-IL|Hayenga's Cant Fi...|small_airport|\n",
      "|     US-KS| Hayden Farm Airport|small_airport|\n",
      "|     US-KY|Robbins Roost Air...|small_airport|\n",
      "+----------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"airportcodes\")\n",
    "ssql = \"\\\n",
    "    SELECT iso_region, name, type \\\n",
    "    FROM airportcodes \\\n",
    "    WHERE iso_country = 'US' \\\n",
    "\"\n",
    "viewdf = spark.sql(ssql)\n",
    "\n",
    "print(viewdf.count())\n",
    "viewdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff5f5f4-51ca-4e39-bcc6-39db134197eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 접속 정보\n",
    "jdbcHostname = \"34.125.136.103\"\n",
    "jdbcDatabase = \"sparkdb\"\n",
    "jdbcPort = 30007\n",
    "jdbcUrl = \"jdbc:mysql://{0}:{1}/{2}?permitMysqlScheme\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n",
    "jdbcTable = \"tb_airport_us\"\n",
    "properties = {\n",
    "  \"driver\" : \"org.mariadb.jdbc.Driver\",\n",
    "  \"user\"   : \"root\",\n",
    "  \"password\" : \"tjehdgml\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b8cd90e-8e50-43f3-bbff-bc37dcd52b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# tb_airport_us이 없을 경우 새로 만든다.\n",
    "# mode: overwrite, append, ignore, error, errorifexists\n",
    "# overwrite: 테이블 싹 지우고 새로 넣기\n",
    "# error/errorifexist: 테이블만 있어도 에러 발\n",
    "viewdf.write.format(\"jdbc\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"url\", jdbcUrl) \\\n",
    "                .option(\"driver\", properties[\"driver\"]) \\\n",
    "                .option(\"dbtable\", jdbcTable) \\\n",
    "                .option(\"user\", properties[\"user\"]) \\\n",
    "                .option(\"password\", properties[\"password\"]) \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "457648b9-8bee-4a79-a876-1d11df287b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"jdbc\") \\\n",
    "                    .option(\"url\", jdbcUrl) \\\n",
    "                    .option(\"driver\", properties[\"driver\"]) \\\n",
    "                    .option(\"dbtable\", jdbcTable) \\\n",
    "                    .option(\"user\", properties[\"user\"]) \\\n",
    "                    .option(\"password\", properties[\"password\"]) \\\n",
    "                    .load()\n",
    "                    \n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0603cd0c-f857-46fe-82e7-6b9f928ecaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate\n",
    "# 결과는 위 셀로 확인\n",
    "viewdf.limit(0).write.format(\"jdbc\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"url\", jdbcUrl) \\\n",
    "                .option(\"driver\", properties[\"driver\"]) \\\n",
    "                .option(\"dbtable\", jdbcTable) \\\n",
    "                .option(\"user\", properties[\"user\"]) \\\n",
    "                .option(\"password\", properties[\"password\"]) \\\n",
    "                .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
