{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90968fca-1265-4eae-9a59-b4f988496dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# boto3를 통해 자동으로 자격증명 가져오기\n",
    "# ~/.aws/credentials를 미리 만들어놔야 함\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "aws_access_key_id = credentials.access_key\n",
    "aws_secret_access_key = credentials.secret_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fcd71a2-4d13-46db-997a-01557fb46a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session with your AWS Credentials\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"sparkQLwithS3\") # replace with your desired name\n",
    "    .set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0,org.apache.hadoop:hadoop-aws:3.3.2\")\n",
    "    .set(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"4\") # default is 200 partitions which is too many for local\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "    # .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.profile.ProfileCredentialsProvider\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", aws_access_key_id)\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_access_key)\n",
    "    .setMaster(\"spark://34.125.136.103:30077\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f968c126-8c04-4f8a-8517-e07cfc4740d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: com.amazonaws.services.s3.enableV4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/root/miniconda3/envs/spark341/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5786531a-3882-4c6d-a1d2-87295e7e113e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 740ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5786531a-3882-4c6d-a1d2-87295e7e113e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/12ms)\n",
      "24/04/25 17:17:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6262301-13b2-48e6-97cd-e8035a19f958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 17:18:02 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('json').load('s3a://jolajoayo-spark-0001/spark2-sql/airports/airport-codes.csv.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb600c26-0d72-4152-9378-d055d7570bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\n",
      "ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+\n",
      "|iso_region|                name|         type|\n",
      "+----------+--------------------+-------------+\n",
      "|     US-PA|   Total Rf Heliport|     heliport|\n",
      "|     US-AK|        Lowell Field|small_airport|\n",
      "|     US-AL|        Epps Airpark|small_airport|\n",
      "|     US-AR|Newport Hospital ...|     heliport|\n",
      "|     US-AZ|      Cordes Airport|small_airport|\n",
      "|     US-CA|Goldstone /Gts/ A...|small_airport|\n",
      "|     US-CO|          Cass Field|small_airport|\n",
      "|     US-FL| Grass Patch Airport|small_airport|\n",
      "|     US-FL|  Ringhaver Heliport|     heliport|\n",
      "|     US-FL|   River Oak Airport|small_airport|\n",
      "|     US-GA|    Lt World Airport|small_airport|\n",
      "|     US-GA|    Caffrey Heliport|     heliport|\n",
      "|     US-HI|  Kaupulehu Heliport|     heliport|\n",
      "|     US-ID|Delta Shores Airport|small_airport|\n",
      "|     US-IN|Bailey Generation...|     heliport|\n",
      "|     US-IL|      Hammer Airport|small_airport|\n",
      "|     US-IN|St Mary Medical C...|     heliport|\n",
      "|     US-IL|Hayenga's Cant Fi...|small_airport|\n",
      "|     US-KS| Hayden Farm Airport|small_airport|\n",
      "|     US-KY|Robbins Roost Air...|small_airport|\n",
      "+----------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"airportcodes\")\n",
    "viewdf = spark.sql(\\\n",
    "                   \"SELECT iso_region, name, type \"\\\n",
    "                   \"FROM airportcodes \"\\\n",
    "                   \"WHERE iso_country = 'US' \"\\\n",
    "                   )\n",
    "\n",
    "viewdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f09eaf12-bedd-4f4c-9266-6e8fb7bb214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707de611-aa9a-490d-9f1d-b4a7a1040d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea27b1a-6e4e-4acf-8f51-cd2be8e21ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95a8bc-bb6b-4de7-ad11-dee1fb5713bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8c7af-7c16-4b1d-b8b7-02db7a2a02ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
