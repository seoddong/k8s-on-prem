{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff4d9d34-dfe5-4834-9946-4dcefc8246ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/root/miniconda3/envs/spark341/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.mariadb.jdbc#mariadb-java-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-53930681-ac63-4cf6-89e1-0d37e3be6f37;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mariadb.jdbc#mariadb-java-client;3.3.3 in central\n",
      "\tfound com.github.waffle#waffle-jna;3.3.0 in central\n",
      "\tfound net.java.dev.jna#jna;5.13.0 in central\n",
      "\tfound net.java.dev.jna#jna-platform;5.13.0 in central\n",
      "\tfound org.slf4j#jcl-over-slf4j;2.0.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.3 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.10.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.32.0 in central\n",
      ":: resolution report :: resolve 815ms :: artifacts dl 27ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.3 from central in [default]\n",
      "\tcom.github.waffle#waffle-jna;3.3.0 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.10.0 from central in [default]\n",
      "\tnet.java.dev.jna#jna;5.13.0 from central in [default]\n",
      "\tnet.java.dev.jna#jna-platform;5.13.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.32.0 from central in [default]\n",
      "\torg.mariadb.jdbc#mariadb-java-client;3.3.3 from central in [default]\n",
      "\torg.slf4j#jcl-over-slf4j;2.0.7 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.checkerframework#checker-qual;3.19.0 by [org.checkerframework#checker-qual;3.32.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   1   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-53930681-ac63-4cf6-89e1-0d37e3be6f37\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/15ms)\n",
      "24/04/29 17:36:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# 다양한 방식으로 MariaDB 테이블 읽고 스기\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "conf = (\n",
    "SparkConf()\n",
    "    .setAppName(\"MY_MariaDB\") # replace with your desired name\n",
    "    .set(\"spark.jars.packages\", \\\n",
    "         \"org.mariadb.jdbc:mariadb-java-client:3.3.3\"\\\n",
    "        )\n",
    "    # .set(\"spark.sql.shuffle.partitions\", \"4\") # default is 200 partitions which is too many for local\n",
    "    # .setMaster(\"local[*]\") # replace the * with your desired number of cores. * for use all.\n",
    "    .setMaster(\"spark://34.125.136.103:30077\")\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69f217b-a113-4e97-8e6e-75f603d134b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 접속 정보\n",
    "jdbcHostname = \"34.125.136.103\"\n",
    "jdbcDatabase = \"sparkdb\"\n",
    "jdbcPort = 30007\n",
    "jdbcUrl = \"jdbc:mysql://{0}:{1}/{2}?permitMysqlScheme\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n",
    "jdbcTable = \"test\"\n",
    "properties = {\n",
    "  \"driver\" : \"org.mariadb.jdbc.Driver\",\n",
    "  \"user\"   : \"root\",\n",
    "  \"password\" : \"tjehdgml\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ff2ec5-760e-49f9-9d74-47fe3632ddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|example|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 데이터 조회\n",
    "# query option은 read에서만 사용 가\n",
    "sql = \"\\\n",
    "    SELECT *\\\n",
    "    FROM   test\\\n",
    "\"\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "                    .option(\"url\", jdbcUrl) \\\n",
    "                    .option(\"driver\", properties[\"driver\"]) \\\n",
    "                    .option(\"query\", sql) \\\n",
    "                    .option(\"user\", properties[\"user\"]) \\\n",
    "                    .option(\"password\", properties[\"password\"]) \\\n",
    "                    .load()\n",
    "                    \n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30377f23-badb-464c-b1c9-d363d3cd1c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJames\u001b[39m\u001b[38;5;124m\"\u001b[39m),(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnn\u001b[39m\u001b[38;5;124m\"\u001b[39m),(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJeff\u001b[39m\u001b[38;5;124m\"\u001b[39m),(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJennifer\u001b[39m\u001b[38;5;124m\"\u001b[39m),(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeoddong\u001b[39m\u001b[38;5;124m\"\u001b[39m),(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEiloy\u001b[39m\u001b[38;5;124m\"\u001b[39m),(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEve\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m---> 13\u001b[0m sampleDF \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m sampleDF\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     17\u001b[0m                 \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, jdbcUrl) \\\n\u001b[1;32m     18\u001b[0m                 \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m     22\u001b[0m                 \u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/miniconda3/envs/spark341/lib/python3.9/site-packages/pyspark/sql/session.py:115\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark341/lib/python3.9/site-packages/pyspark/sql/session.py:1276\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1275\u001b[0m     )\n\u001b[0;32m-> 1276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark341/lib/python3.9/site-packages/pyspark/sql/session.py:1316\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m-> 1316\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1318\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark341/lib/python3.9/site-packages/pyspark/sql/session.py:931\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 931\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    933\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark341/lib/python3.9/site-packages/pyspark/sql/session.py:882\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    880\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m samplingRatio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rdd\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m100\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[0;32m~/miniconda3/envs/spark341/lib/python3.9/site-packages/pyspark/sql/types.py:1566\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(row\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[1;32m   1568\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "# MariaDB에 새 테이블 생성 및 데이터 삽입\n",
    "# sql = \"\n",
    "#     CREATE TABLE IF NOT EXISTS test (\\\n",
    "#         id INT AUTO_INCREMENT PRIMARY KEY,\\\n",
    "#         name VARCHAR(255)\\\n",
    "#     )\\\n",
    "# \"\n",
    "\n",
    "# 데이터 삽입\n",
    "columns = [\"name\"]\n",
    "data = [(\"James\"),(\"Ann\"),(\"Jeff\"),(\"Jennifer\"),(\"Seoddong\"),(\"Eiloy\"),(\"Eve\")]\n",
    "\n",
    "sampleDF = spark.sparkContext.parallelize(data).toDF(columns)\n",
    "\n",
    "\n",
    "sampleDF.write.format(\"jdbc\") \\\n",
    "                .option(\"url\", jdbcUrl) \\\n",
    "                .option(\"driver\", properties[\"driver\"]) \\\n",
    "                .option(\"dbtable\", jdbcTable) \\\n",
    "                .option(\"user\", properties[\"user\"]) \\\n",
    "                .option(\"password\", properties[\"password\"]) \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d03ec5cd-6e88-4f97-b6d4-4d551d1cb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 세션 종료\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
