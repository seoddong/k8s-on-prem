from pyspark import SparkConf
from pyspark.sql import SparkSession


spark.stop()



conf = SparkConf()
conf.setAll([
    ("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.4.0," \
            "com.amazonaws:aws-java-sdk-bundle:1.12.696," \
            "org.apache.hadoop:hadoop-common:3.4.0" \
    ), \
    ("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem"),
    ("com.amazonaws.services.s3.enableV4", "true"),
    ("spark.hadoop.fs.s3a.endpoint", "s3.amazonaws.com"),
    ("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.profile.ProfileCredentialsProvider")
])

spark = SparkSession.builder \
    .master("local[*]") \
    .config(conf=conf) \
    .config("spark.hadoop.fs.s3a.access.key", "aaaaaaaaaaaaaaaaaaaaaaaa") \
    .config("spark.hadoop.fs.s3a.secret.key", "bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb") \
    .appName("read-s3-with-spark") \
    .getOrCreate()

df = spark.read \
    .option("header", "true") \
    .csv("s3a://nerdward-bucket/read-s3-with-spark/top-50-yt-channels-20220912-20220918.csv")
    # note the use of s3a here too

df.select("Rank", "Channel", "Country", "Channel Type", "Number of Subscribers(In Millions)").show(truncate=False)


print(spark.sparkContext._conf.getAll())


import pyspark
print(pyspark.__version__)
print(pyspark.__file__)

