import os
import sys
os.environ["PATH"]


import os
import sys
os.environ["SPARK_HOME"] = "/root/miniconda3/envs/myenv/lib/python3.11/site-packages/pyspark"

!wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.696/aws-java-sdk-bundle-1.12.696.jar -P $SPARK_HOME/jars/
!wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.0/hadoop-aws-3.4.0.jar -P $SPARK_HOME/jars/


from pyspark import SparkConf, SparkContext
from pyspark.sql import SQLContext, SparkSession



# SparkSession을 사용하여 Spark SQL 환경을 초기화합니다.
spark = SparkSession.builder \
    .master("spark://34.125.136.103:30077") \
    .appName("MyAppWithS3Access") \
    .config("spark.kubernetes.namespace", "default") \
    .config("spark.jars.packages",
            "org.apache.hadoop:hadoop-aws:3.4.0," \
            "com.amazonaws:aws-java-sdk-bundle:1.12.696") \
    .config("spark.hadoop.fs.s3a.access.key", "aaaaaaaaaaaaaaaaaa") \
    .config("spark.hadoop.fs.s3a.secret.key", "bbbbbbbbbbbbbbbbbbbbbbbbbbbb") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.endpoint", "s3.amazonaws.com") \
    .getOrCreate()


# 데이터를 읽어 DataFrame을 생성합니다.
df = spark.read.json("s3a://jolajoayo-spark-0001/spark2-sql/airports/airport-codes.csv.json")







# import pandas as pd
# df = pd.read_json('people.json', lines=True)
df = spark.createDataFrame(df)

# DataFrame을 임시 뷰로 등록합니다.
df.createOrReplaceTempView("airportcodes")

# SQL 쿼리를 사용하여 데이터를 처리합니다.
# sqlDF = spark.sql("SELECT name, age FROM people WHERE age > 21")
sqlDF = spark.sql("SELECT top 10 * FROM airportcodes")

# 결과를 출력합니다.
sqlDF.show()



# SQL 쿼리를 사용하여 데이터를 처리합니다.
sqlDF = spark.sql("SELECT name, age FROM people")

# 결과를 출력합니다.
sqlDF.show()



spark.stop()
